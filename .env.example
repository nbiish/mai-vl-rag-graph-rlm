# API Keys for Unified RLM
# Get keys from respective provider dashboards

# =====================================================
# Provider Hierarchy (fallback order for 'auto' mode)
# =====================================================
# When --provider is omitted or set to 'auto', providers are tried in this order.
# Only providers with valid API keys are attempted. First available wins.
# Edit this comma-separated list to customize the fallback order.
# Run: vrlmrag --show-hierarchy  to see current status.
# NOTE: If OPENAI_COMPATIBLE_API_KEY or ANTHROPIC_COMPATIBLE_API_KEY is set,
# those providers are automatically prepended to the hierarchy (highest priority).
PROVIDER_HIERARCHY=sambanova,nebius,groq,cerebras,zai,zenmux,openrouter,gemini,deepseek,openai,anthropic,mistral,fireworks,together,azure_openai

# =====================================================
# Fallback API Key System (Multi-Account Support)
# =====================================================
# Every provider supports a FALLBACK API key for multi-account resilience.
# When the primary key fails (rate limits, auth errors, credits exhausted),
# the system automatically retries with the fallback key before moving to
# the next provider in the hierarchy.
#
# Pattern: {PROVIDER}_API_KEY_FALLBACK=your_second_account_key
#
# Example: Two OpenRouter accounts for credit distribution:
#   OPENROUTER_API_KEY=sk-or-v1-primary-account-key
#   OPENROUTER_API_KEY_FALLBACK=sk-or-v1-secondary-account-key
#
# The fallback key uses the same provider endpoint and model — only the
# API key (account) changes. This is different from model fallback
# ({PROVIDER}_FALLBACK_MODEL) which changes the model on the same key.
#
# Three-tier resilience per provider:
#   1. Primary key + primary model
#   2. Fallback key + primary model  (same provider, different account)
#   3. Primary key + fallback model  (same provider, different model)
#   4. Provider hierarchy fallback   (next provider entirely)

# =====================================================
# OpenAI-Compatible Providers (Generic)
# =====================================================
# Any provider with OpenAI-compatible API (custom base URL)
# Examples: Groq, Mistral, Fireworks, Together, DeepSeek, etc.
OPENAI_COMPATIBLE_API_KEY=your_api_key_here
# OPENAI_COMPATIBLE_API_KEY_FALLBACK=your_second_account_key_here
OPENAI_COMPATIBLE_BASE_URL=https://api.example.com/v1
OPENAI_COMPATIBLE_MODEL=your-model-name

# =====================================================
# OpenAI
# =====================================================
OPENAI_API_KEY=your_openai_api_key_here
# OPENAI_API_KEY_FALLBACK=your_second_openai_key_here
OPENAI_BASE_URL=https://api.openai.com/v1  # Optional: for custom endpoints
# OPENAI_MODEL=gpt-4o-mini  # Optional: defaults to gpt-4o-mini
# OPENAI_RECURSIVE_MODEL=gpt-4o-mini  # Optional: defaults to main model (gpt-4o-mini) if not set

# =====================================================
# Azure OpenAI
# =====================================================
AZURE_OPENAI_API_KEY=your_azure_key_here
# AZURE_OPENAI_API_KEY_FALLBACK=your_second_azure_key_here
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
AZURE_OPENAI_API_VERSION=2024-02-01
# AZURE_OPENAI_MODEL=gpt-4o  # Optional: set your deployed model name
# AZURE_OPENAI_RECURSIVE_MODEL=gpt-4o  # Optional: defaults to main model if not set

# =====================================================
# Anthropic
# =====================================================
ANTHROPIC_API_KEY=your_anthropic_api_key_here
# ANTHROPIC_API_KEY_FALLBACK=your_second_anthropic_key_here
ANTHROPIC_BASE_URL=https://api.anthropic.com  # Optional: for custom endpoints/proxies
# ANTHROPIC_MODEL=claude-4.5-haiku  # Optional: defaults to claude-3-5-haiku
# ANTHROPIC_RECURSIVE_MODEL=claude-3-5-haiku-20241022  # Optional: defaults to main model if not set

# =====================================================
# Anthropic-Compatible Providers (Generic)
# =====================================================
# Any provider with Anthropic-compatible API
ANTHROPIC_COMPATIBLE_API_KEY=your_api_key_here
# ANTHROPIC_COMPATIBLE_API_KEY_FALLBACK=your_second_account_key_here
ANTHROPIC_COMPATIBLE_BASE_URL=https://api.example.com
ANTHROPIC_COMPATIBLE_MODEL=your-model-name

# =====================================================
# OpenRouter
# =====================================================
OPENROUTER_API_KEY=your_openrouter_api_key_here
# OPENROUTER_API_KEY_FALLBACK=your_second_openrouter_key_here
# OPENROUTER_MODEL=upstage/solar-pro-3:free  # Optional: defaults to minimax/minimax-m2.1
# OPENROUTER_RECURSIVE_MODEL=openrouter/pony-alpha  # Optional: defaults to main model if not set

# =====================================================
# ZenMux (unified API gateway — https://docs.zenmux.ai)
# Models use provider/model-name format
# OpenAI protocol: https://zenmux.ai/api/v1
# Anthropic protocol: https://zenmux.ai/api/anthropic
# =====================================================
ZENMUX_API_KEY=your_zenmux_api_key_here
# ZENMUX_API_KEY_FALLBACK=your_second_zenmux_key_here
# ZENMUX_MODEL=moonshotai/kimi-k2.5  # Optional: defaults to moonshotai/kimi-k2.5
# ZENMUX_FALLBACK_MODEL=z-ai/glm-4.7  # Auto-fallback on any primary model error
# ZENMUX_RECURSIVE_MODEL=z-ai/glm-4.7-flash  # Optional: defaults to main model if not set

# =====================================================
# z.ai (Zhipu AI - GLM series)
# Coding Plan ($3-15/mo): https://api.z.ai/api/coding/paas/v4
# Normal (pay-per-token): https://open.bigmodel.cn/api/paas/v4
# Coding Plan is tried first; falls back to normal on failure.
# Set ZAI_CODING_PLAN=false to skip Coding Plan endpoint.
# Docs: https://docs.z.ai/devpack/tool/others
# =====================================================
ZAI_API_KEY=your_zai_api_key_here
# ZAI_API_KEY_FALLBACK=your_second_zai_key_here
# ZAI_CODING_PLAN=true  # Optional: try Coding Plan endpoint first (default: true)
# ZAI_MODEL=glm-4.7  # Optional: defaults to glm-4.7
# ZAI_FALLBACK_MODEL=glm-4.5-air  # Auto-fallback on any primary model error
# ZAI_RECURSIVE_MODEL=glm-4.5-air  # Optional: defaults to main model if not set

# =====================================================
# Google
# =====================================================
GOOGLE_API_KEY=your_google_api_key_here
# GOOGLE_API_KEY_FALLBACK=your_second_google_key_here
# GOOGLE_MODEL=gemini-3-pro  # Optional: defaults to gemini-1.5-flash
# GOOGLE_RECURSIVE_MODEL=gemini-3-flash  # Optional: defaults to main model if not set

# =====================================================
# Other Popular Providers
# =====================================================

# Groq (ultra-fast LPU inference — https://console.groq.com/docs/models)
GROQ_API_KEY=your_groq_api_key_here
# GROQ_API_KEY_FALLBACK=your_second_groq_key_here
# GROQ_MODEL=moonshotai/kimi-k2-instruct-0905  # Optional: defaults to moonshotai/kimi-k2-instruct-0905
# GROQ_RECURSIVE_MODEL=llama-3.3-70b-versatile  # Optional: defaults to main model if not set
# GROQ_FALLBACK_MODEL=llama-3.3-70b-versatile  # Auto-fallback on any primary model error
# Other options: openai/gpt-oss-120b, meta-llama/llama-4-maverick-17b-128e-instruct,
#   llama-3.3-70b-versatile, qwen/qwen3-32b, groq/compound

# Cerebras (ultra-fast wafer-scale inference — https://inference-docs.cerebras.ai)
CEREBRAS_API_KEY=your_cerebras_api_key_here
# CEREBRAS_API_KEY_FALLBACK=your_second_cerebras_key_here
# CEREBRAS_MODEL=zai-glm-4.7  # Optional: defaults to zai-glm-4.7 (355B, ~1000 tok/s)
# CEREBRAS_RECURSIVE_MODEL=gpt-oss-120b  # Optional: defaults to main model if not set
# CEREBRAS_FALLBACK_MODEL=gpt-oss-120b  # Auto-fallback on any primary model error
# Other options: gpt-oss-120b (~3000 tok/s), qwen-3-235b-a22b-instruct-2507 (~1400 tok/s)
# NOTE: llama-3.3-70b and qwen-3-32b deprecated Feb 16, 2026

# Mistral AI
MISTRAL_API_KEY=your_mistral_api_key_here
# MISTRAL_API_KEY_FALLBACK=your_second_mistral_key_here
# MISTRAL_MODEL=mistral-large-latest  # Optional: set your preferred model
# MISTRAL_RECURSIVE_MODEL=mistral-small-latest  # Optional: defaults to main model if not set
# MISTRAL_FALLBACK_MODEL=mistral-small-latest  # Auto-fallback on any primary model error

# Fireworks AI
FIREWORKS_API_KEY=your_fireworks_api_key_here
# FIREWORKS_API_KEY_FALLBACK=your_second_fireworks_key_here
# FIREWORKS_MODEL=accounts/fireworks/models/llama-v3p1-70b-instruct  # Optional
# FIREWORKS_RECURSIVE_MODEL=accounts/fireworks/models/mixtral-8x22b-instruct  # Optional: defaults to main model if not set
# FIREWORKS_FALLBACK_MODEL=accounts/fireworks/models/mixtral-8x22b-instruct  # Auto-fallback

# Together AI
TOGETHER_API_KEY=your_together_api_key_here
# TOGETHER_API_KEY_FALLBACK=your_second_together_key_here
# TOGETHER_MODEL=meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo  # Optional
# TOGETHER_RECURSIVE_MODEL=mistralai/Mixtral-8x22B-Instruct-v0.1  # Optional: defaults to main model if not set
# TOGETHER_FALLBACK_MODEL=mistralai/Mixtral-8x22B-Instruct-v0.1  # Auto-fallback

# DeepSeek
DEEPSEEK_API_KEY=your_deepseek_api_key_here
# DEEPSEEK_API_KEY_FALLBACK=your_second_deepseek_key_here
# DEEPSEEK_MODEL=deepseek-chat  # Optional: defaults to deepseek-chat
# DEEPSEEK_RECURSIVE_MODEL=deepseek-chat  # Optional: defaults to main model if not set
# DEEPSEEK_FALLBACK_MODEL=deepseek-reasoner  # Auto-fallback on any primary model error

# =====================================================
# SambaNova Cloud
# =====================================================
SAMBANOVA_API_KEY=your_sambanova_api_key_here
# SAMBANOVA_API_KEY_FALLBACK=your_second_sambanova_key_here
# SAMBANOVA_MODEL=DeepSeek-V3-0324  # Optional: defaults to DeepSeek-V3-0324 (32K context)
# SAMBANOVA_RECURSIVE_MODEL=DeepSeek-V3.1  # Optional: defaults to main model if not set
# SAMBANOVA_FALLBACK_MODEL=DeepSeek-V3.1  # Auto-fallback on any error (32K+ context)
# ⚠️  DO NOT use DeepSeek-V3.2 — it has only 8,192 token context (too small for RAG)
# Other options: DeepSeek-V3.1, gpt-oss-120b, Qwen3-235B-A22B-Instruct-2507, Llama-4-Maverick-17B-128E-Instruct

# =====================================================
# Nebius Token Factory
# =====================================================
# Nebius advantage: NO daily token limits (unlike SambaNova's 200K TPD)
# Get API key: https://tokenfactory.nebius.com
NEBIUS_API_KEY=your_nebius_api_key_here
# NEBIUS_API_KEY_FALLBACK=your_second_nebius_key_here
# NEBIUS_MODEL=MiniMaxAI/MiniMax-M2.1  # Optional: defaults to MiniMaxAI/MiniMax-M2.1
# NEBIUS_RECURSIVE_MODEL=zai-org/GLM-4.7-FP8  # Optional: defaults to main model if not set
# NEBIUS_FALLBACK_MODEL=zai-org/GLM-4.7-FP8  # Auto-fallback on any primary model error
# Other options: zai-org/GLM-4.7-FP8, deepseek-ai/DeepSeek-R1-0528,
#   nvidia/Llama-3.1-Nemotron-Ultra-253B-v1, moonshotai/Kimi-K2-Instruct
# NEBIUS_CONTEXT_WINDOW=128000  # Optional: context window in tokens (default: 128000)
# With 128K tokens, CLI can use ~100K char context vs ~8K for SambaNova

# =====================================================
# Modal Research (experimental GLM-5 inference)
# =====================================================
# Modal Research provides free OpenAI-compatible inference for GLM-5,
# Zhipu AI's 745B MoE frontier model (44B active, MIT license).
# Runs on 8×B200 GPUs via SGLang with 30-75 tok/s per user.
# ⚠️  Experimental: 1 concurrent request per credential, may have downtime.
# Get API key: https://modal.com/glm-5-endpoint
# Blog: https://modal.com/blog/try-glm-5
MODAL_RESEARCH_API_KEY=your_modal_research_key_here
# MODAL_RESEARCH_API_KEY_FALLBACK=your_second_modal_research_key_here
# MODAL_RESEARCH_MODEL=zai-org/GLM-5-FP8  # Optional: defaults to zai-org/GLM-5-FP8
# MODAL_RESEARCH_RECURSIVE_MODEL=zai-org/GLM-5-FP8  # Optional: defaults to main model if not set

# =====================================================
# Ollama (Local LLM Inference OR Claude API Mode)
# =====================================================
# Ollama supports TWO modes of operation:
#
# 1. LOCAL MODE (default): Uses local Ollama installation
#    - Requires Ollama installed and running: https://ollama.com
#    - Default URL: http://localhost:11434
#    - No API keys needed
#
# 2. API MODE: Uses Claude models via Ollama's API compatibility layer
#    - Set OLLAMA_MODE=api and provide OLLAMA_API_KEY
#    - Uses Claude models but through Ollama interface
#    - Acts like an API provider (not local inference)
#
# Set OLLAMA_ENABLED=true to include Ollama in the provider hierarchy.
# When enabled, Ollama acts as a fallback after API providers fail.
#
# Common local models (install with `ollama pull <model>`):
# - llama3.2 (3B, fast, good for recursive calls)
# - llama3.1 (8B, good balance)
# - mistral (7B, efficient)
# - qwen2.5:7b (strong reasoning)
# - deepseek-r1 (reasoning model)
#
# OLLAMA_ENABLED=true
# OLLAMA_MODE=local  # Options: local, api (default: local)
OLLAMA_BASE_URL=http://localhost:11434
# OLLAMA_MODEL=llama3.2  # Optional: defaults to llama3.2 (or Claude model in API mode)
# OLLAMA_API_KEY=your_claude_api_key_here  # Required when OLLAMA_MODE=api
# OLLAMA_API_KEY_FALLBACK=your_fallback_ollama_key_here  # Secondary key for multi-account support
# OLLAMA_RECURSIVE_MODEL=llama3.2  # Optional: defaults to main model if not set

# =====================================================
# Model Configuration
# =====================================================
# All model names used by the system can be overridden here.
# Change these to swap models without touching code.

# -- Text-only embedding model (used when VRLMRAG_TEXT_ONLY is true) --
# Lightweight text-only model (~1.2 GB for 0.6B, ~8 GB for 8B)
# Options: Qwen/Qwen3-Embedding-0.6B, Qwen/Qwen3-Embedding-4B, Qwen/Qwen3-Embedding-8B
# VRLMRAG_TEXT_ONLY_MODEL=Qwen/Qwen3-Embedding-0.6B

# -- Multimodal embedding model (used when both flags are false) --
# Local VL embedding model (loads into GPU/CPU, ~4.6 GB)
# VRLMRAG_LOCAL_EMBEDDING_MODEL=Qwen/Qwen3-VL-Embedding-2B

# FlashRank reranker model (ONNX, ~4-150 MB depending on tier)
# Options: ms-marco-TinyBERT-L-2-v2 (~4 MB), ms-marco-MiniLM-L-12-v2 (~34 MB),
#          rank-T5-flan (~110 MB), ms-marco-MultiBERT-L-12 (~150 MB, multilingual)
# VRLMRAG_RERANKER_MODEL=ms-marco-MiniLM-L-12-v2

# =====================================================
# Embedding Mode Toggle
# =====================================================
# Three mutually exclusive modes (first match wins):
#
# 1. TEXT_ONLY (lightest — ~1.2 GB RAM, text only, fully offline)
#    Set VRLMRAG_TEXT_ONLY=true  — uses Qwen3-Embedding for text-only RAG.
#    Skips image/video embedding. Ideal for .txt, .md, text-heavy PDFs.
#
# 2. API (DEFAULT — ~200 MB RAM, requires internet)
#    API mode is the default. Uses OpenRouter + ZenMux omni APIs.
#    Requires OPENROUTER_API_KEY + ZENMUX_API_KEY above.
#    Video/audio files ALWAYS use API mode (local is blocked for safety).
#
# 3. LOCAL (opt-in — ~4.6 GB RAM, fully offline)
#    Set VRLMRAG_LOCAL=true or use --local flag on CLI.
#    Uses local Qwen3-VL-Embedding-2B + FlashRank.
#    Handles text, images, PowerPoints, PDFs with figures.
#    ⚠️  Local mode is BLOCKED for video/audio files to prevent OOM crashes.
# VRLMRAG_TEXT_ONLY=false
# VRLMRAG_LOCAL=false

# -- API embedding provider (OpenRouter) -- used when VRLMRAG_USE_API=true
# Uses OPENROUTER_API_KEY (above) by default. Override with:
# VRLMRAG_EMBEDDING_API_KEY=your_openrouter_api_key_here
# VRLMRAG_EMBEDDING_BASE_URL=https://openrouter.ai/api/v1
# VRLMRAG_EMBEDDING_MODEL=openai/text-embedding-3-small

# =====================================================
# Omni Models (Multimodal: Text + Image + Audio + Video)
# =====================================================
# Omni models handle ALL multimodal content: images, audio files, video frames, and text.
# They are used for describing visual content and transcribing audio in API mode.
# Three-tier fallback chain: Primary → Secondary → Tertiary
#
# ┌─────────────────────────────────────────────────────────────────────────────┐
# │  PRIMARY OMNI MODEL (ZenMux Ming-flash-omni-preview)                        │
# │  ├── Supports: text, images, audio, video                                   │
# │  ├── Provider: ZenMux (ZENMUX_API_KEY)                                      │
# │  └── Base URL: https://zenmux.ai/api/v1                                     │
# └─────────────────────────────────────────────────────────────────────────────┘
# VRLMRAG_OMNI_API_KEY=your_zenmux_api_key_here       # Optional: defaults to ZENMUX_API_KEY
# VRLMRAG_OMNI_BASE_URL=https://zenmux.ai/api/v1
# VRLMRAG_OMNI_MODEL=inclusionai/ming-flash-omni-preview
#
# ┌─────────────────────────────────────────────────────────────────────────────┐
# │  SECONDARY OMNI FALLBACK (ZenMux Gemini 3 Flash Preview)                     │
# │  ├── Used when: Primary omni fails or returns errors                        │
# │  ├── Supports: text, images, audio, video                                   │
# │  ├── Provider: ZenMux (same key as primary)                                 │
# │  └── Base URL: https://zenmux.ai/api/v1                                     │
# └─────────────────────────────────────────────────────────────────────────────┘
# VRLMRAG_OMNI_FALLBACK_API_KEY=your_zenmux_api_key_here  # Optional: defaults to ZENMUX_API_KEY
# VRLMRAG_OMNI_FALLBACK_BASE_URL=https://zenmux.ai/api/v1
# VRLMRAG_OMNI_FALLBACK_MODEL=gemini/gemini-3-flash-preview
#
# ┌─────────────────────────────────────────────────────────────────────────────┐
# │  TERTIARY OMNI FALLBACK (OpenRouter Gemini 3 Flash Preview)                │
# │  ├── Used when: Both primary and secondary omnis fail                     │
# │  ├── Supports: text, images, audio, video                                   │
# │  ├── Provider: OpenRouter (OPENROUTER_API_KEY)                              │
# │  └── Base URL: https://openrouter.ai/api/v1                                 │
# └─────────────────────────────────────────────────────────────────────────────┘
# VRLMRAG_OMNI_FALLBACK_API_KEY_2=your_openrouter_api_key_here  # Optional: defaults to OPENROUTER_API_KEY
# VRLMRAG_OMNI_FALLBACK_BASE_URL_2=https://openrouter.ai/api/v1
# VRLMRAG_OMNI_FALLBACK_MODEL_2=google/gemini-3-flash-preview
#
# ┌─────────────────────────────────────────────────────────────────────────────┐
# │  VLM FALLBACK (OpenRouter Kimi K2.5) — Images/Video ONLY                    │
# │  ├── Used when: All three omni models fail                                  │
# │  ├── Supports: text, images, video (NO audio support)                       │
# │  ├── Provider: OpenRouter (OPENROUTER_API_KEY)                              │
# │  └── Note: Only used for image/video, NOT audio transcription               │
# └─────────────────────────────────────────────────────────────────────────────┘
# VRLMRAG_VLM_FALLBACK_API_KEY=your_openrouter_api_key_here  # Optional: defaults to OPENROUTER_API_KEY
# VRLMRAG_VLM_FALLBACK_BASE_URL=https://openrouter.ai/api/v1
# VRLMRAG_VLM_FALLBACK_MODEL=moonshotai/kimi-k2.5

# =====================================================
# Audio / Video Processing
# =====================================================
# Two modes for media files (.mp4, .wav, .mp3, .flac, .m4a, etc.):
#
# LOCAL (VRLMRAG_USE_API=false):
#   - Audio: Parakeet ASR transcribes audio → text → Qwen3-VL embeds text
#   - Video: ffmpeg extracts audio + key frames → Parakeet + Qwen3-VL
#   - Requires: pip install nemo_toolkit[asr]  (NVIDIA Parakeet ~0.6 GB)
#   - Requires: ffmpeg installed (brew install ffmpeg / apt install ffmpeg)
#
# API (VRLMRAG_USE_API=true):
#   - Audio/Video: ZenMux omni model describes frames → text → API embeds
#   - Uses VRLMRAG_VLM_MODEL (default: inclusionai/ming-flash-omni-preview)
#   - No local models needed — all processing via API
#
# Parakeet model override (local mode only):
# VRLMRAG_PARAKEET_MODEL=nvidia/parakeet-tdt-0.6b-v3
#
# Video frame extraction rate (frames per second, lower = fewer frames):
# VRLMRAG_VIDEO_FPS=0.5
# VRLMRAG_VIDEO_MAX_FRAMES=32

# =====================================================
# Timeout Configuration (for Long-Term Thinking Models)
# =====================================================
# Long-term thinking/reasoning models (DeepSeek-R1, o1, o3, GLM-5, etc.)
# can take 30s-600s to complete complex reasoning tasks.
# 
# Default timeouts:
#   - Normal models: 120 seconds (2 minutes)
#   - Reasoning models: 300 seconds (5 minutes)
#   - Maximum allowed: 600 seconds (10 minutes)
#
# Override with environment variables:
# VRLMRAG_TIMEOUT=180  # Set timeout for ALL models (normal and reasoning)
# VRLMRAG_REASONING_TIMEOUT=600  # Set timeout for reasoning models only
#
# Recognized reasoning models include:
#   - deepseek-r1, deepseek-reasoner, deepseek-r1-0528
#   - o1, o1-preview, o1-mini, o3, o3-mini
#   - glm-5, z-ai/glm-5, kimi-k1.5, ernie-5.0-thinking
#   - groq/compound
#
# The system auto-detects reasoning models by name pattern (-r1, -reasoner,
# -thinking, o1-, o3-, compound) or exact match.
# VRLMRAG_TIMEOUT=120
# VRLMRAG_REASONING_TIMEOUT=300

# =====================================================
# HuggingFace
# =====================================================
# HuggingFace token for downloading models (optional)
HF_TOKEN=your_huggingface_token_here

# =====================================================
# LiteLLM Universal Client (supports 100+ providers)
# =====================================================
# LiteLLM provides a unified interface to 100+ LLM providers.
# Set a fallback key for multi-account support across all LiteLLM providers.
# LITELLM_API_KEY=your_litellm_key_here  # If using LiteLLM proxy/redirect
# LITELLM_API_KEY_FALLBACK=your_fallback_litellm_key_here  # Secondary key for multi-account support
