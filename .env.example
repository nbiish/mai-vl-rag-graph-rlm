# API Keys for Unified RLM
# Get keys from respective provider dashboards

# =====================================================
# Provider Hierarchy (fallback order for 'auto' mode)
# =====================================================
# When --provider is omitted or set to 'auto', providers are tried in this order.
# Only providers with valid API keys are attempted. First available wins.
# Edit this comma-separated list to customize the fallback order.
# Run: vrlmrag --show-hierarchy  to see current status.
# NOTE: If OPENAI_COMPATIBLE_API_KEY or ANTHROPIC_COMPATIBLE_API_KEY is set,
# those providers are automatically prepended to the hierarchy (highest priority).
PROVIDER_HIERARCHY=sambanova,nebius,groq,cerebras,zai,zenmux,openrouter,gemini,deepseek,openai,anthropic,mistral,fireworks,together,azure_openai

# =====================================================
# OpenAI-Compatible Providers (Generic)
# =====================================================
# Any provider with OpenAI-compatible API (custom base URL)
# Examples: Groq, Mistral, Fireworks, Together, DeepSeek, etc.
OPENAI_COMPATIBLE_API_KEY=your_api_key_here
OPENAI_COMPATIBLE_BASE_URL=https://api.example.com/v1
OPENAI_COMPATIBLE_MODEL=your-model-name

# =====================================================
# OpenAI
# =====================================================
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_BASE_URL=https://api.openai.com/v1  # Optional: for custom endpoints
# OPENAI_MODEL=gpt-4o-mini  # Optional: defaults to gpt-4o-mini

# =====================================================
# Azure OpenAI
# =====================================================
AZURE_OPENAI_API_KEY=your_azure_key_here
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
AZURE_OPENAI_API_VERSION=2024-02-01
# AZURE_OPENAI_MODEL=gpt-4o  # Optional: set your deployed model name

# =====================================================
# Anthropic
# =====================================================
ANTHROPIC_API_KEY=your_anthropic_api_key_here
ANTHROPIC_BASE_URL=https://api.anthropic.com  # Optional: for custom endpoints/proxies
# ANTHROPIC_MODEL=claude-4.5-haiku  # Optional: defaults to claude-3-5-haiku

# =====================================================
# Anthropic-Compatible Providers (Generic)
# =====================================================
# Any provider with Anthropic-compatible API
ANTHROPIC_COMPATIBLE_API_KEY=your_api_key_here
ANTHROPIC_COMPATIBLE_BASE_URL=https://api.example.com
ANTHROPIC_COMPATIBLE_MODEL=your-model-name

# =====================================================
# OpenRouter
# =====================================================
OPENROUTER_API_KEY=your_openrouter_api_key_here
# OPENROUTER_MODEL=upstage/solar-pro-3:free  # Optional: defaults to minimax/minimax-m2.1
# OPENROUTER_RECURSIVE_MODEL=openrouter/pony-alpha  # Optional: for recursive calls

# =====================================================
# ZenMux (unified API gateway — https://docs.zenmux.ai)
# Models use provider/model-name format
# OpenAI protocol: https://zenmux.ai/api/v1
# Anthropic protocol: https://zenmux.ai/api/anthropic
# =====================================================
ZENMUX_API_KEY=your_zenmux_api_key_here
# ZENMUX_MODEL=moonshotai/kimi-k2.5  # Optional: defaults to moonshotai/kimi-k2.5
# ZENMUX_FALLBACK_MODEL=z-ai/glm-4.7  # Auto-fallback on any primary model error
# ZENMUX_RECURSIVE_MODEL=z-ai/glm-4.7-flash  # Optional: for recursive calls

# =====================================================
# z.ai (Zhipu AI - GLM series)
# Coding Plan ($3-15/mo): https://api.z.ai/api/coding/paas/v4
# Normal (pay-per-token): https://open.bigmodel.cn/api/paas/v4
# Coding Plan is tried first; falls back to normal on failure.
# Set ZAI_CODING_PLAN=false to skip Coding Plan endpoint.
# Docs: https://docs.z.ai/devpack/tool/others
# =====================================================
ZAI_API_KEY=your_zai_api_key_here
# ZAI_CODING_PLAN=true  # Optional: try Coding Plan endpoint first (default: true)
# ZAI_MODEL=glm-4.7  # Optional: defaults to glm-4.7
# ZAI_FALLBACK_MODEL=glm-4.5-air  # Auto-fallback on any primary model error
# ZAI_RECURSIVE_MODEL=glm-4.5-air  # Optional: for recursive calls (glm-4.5-air is fast + on Coding Plan)

# =====================================================
# Google
# =====================================================
GOOGLE_API_KEY=your_google_api_key_here
# GOOGLE_MODEL=gemini-3-pro  # Optional: defaults to gemini-1.5-flash
# GOOGLE_RECURSIVE_MODEL=gemini-3-flash  # Optional: for recursive calls

# =====================================================
# Other Popular Providers
# =====================================================

# Groq (ultra-fast LPU inference — https://console.groq.com/docs/models)
GROQ_API_KEY=your_groq_api_key_here
# GROQ_MODEL=moonshotai/kimi-k2-instruct-0905  # Optional: defaults to moonshotai/kimi-k2-instruct-0905
# GROQ_FALLBACK_MODEL=llama-3.3-70b-versatile  # Auto-fallback on any primary model error
# Other options: openai/gpt-oss-120b, meta-llama/llama-4-maverick-17b-128e-instruct,
#   llama-3.3-70b-versatile, qwen/qwen3-32b, groq/compound

# Cerebras (ultra-fast wafer-scale inference — https://inference-docs.cerebras.ai)
CEREBRAS_API_KEY=your_cerebras_api_key_here
# CEREBRAS_MODEL=zai-glm-4.7  # Optional: defaults to zai-glm-4.7 (355B, ~1000 tok/s)
# CEREBRAS_FALLBACK_MODEL=gpt-oss-120b  # Auto-fallback on any primary model error
# Other options: gpt-oss-120b (~3000 tok/s), qwen-3-235b-a22b-instruct-2507 (~1400 tok/s)
# NOTE: llama-3.3-70b and qwen-3-32b deprecated Feb 16, 2026

# Mistral AI
MISTRAL_API_KEY=your_mistral_api_key_here
# MISTRAL_MODEL=mistral-large-latest  # Optional: set your preferred model
# MISTRAL_FALLBACK_MODEL=mistral-small-latest  # Auto-fallback on any primary model error

# Fireworks AI
FIREWORKS_API_KEY=your_fireworks_api_key_here
# FIREWORKS_MODEL=accounts/fireworks/models/llama-v3p1-70b-instruct  # Optional
# FIREWORKS_FALLBACK_MODEL=accounts/fireworks/models/mixtral-8x22b-instruct  # Auto-fallback

# Together AI
TOGETHER_API_KEY=your_together_api_key_here
# TOGETHER_MODEL=meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo  # Optional
# TOGETHER_FALLBACK_MODEL=mistralai/Mixtral-8x22B-Instruct-v0.1  # Auto-fallback

# DeepSeek
DEEPSEEK_API_KEY=your_deepseek_api_key_here
# DEEPSEEK_MODEL=deepseek-chat  # Optional: defaults to deepseek-chat
# DEEPSEEK_FALLBACK_MODEL=deepseek-reasoner  # Auto-fallback on any primary model error

# =====================================================
# SambaNova Cloud
# =====================================================
SAMBANOVA_API_KEY=your_sambanova_api_key_here
# SAMBANOVA_MODEL=DeepSeek-V3.2  # Optional: defaults to DeepSeek-V3.2
# SAMBANOVA_FALLBACK_MODEL=DeepSeek-V3.1  # Auto-fallback on any V3.2 error
# Other options: DeepSeek-V3.1, gpt-oss-120b, Qwen3-235B, Llama-4-Maverick-17B-128E-Instruct

# =====================================================
# Nebius Token Factory
# =====================================================
# Nebius advantage: NO daily token limits (unlike SambaNova's 200K TPD)
# Get API key: https://tokenfactory.nebius.com
NEBIUS_API_KEY=your_nebius_api_key_here
# NEBIUS_MODEL=MiniMaxAI/MiniMax-M2.1  # Optional: defaults to MiniMaxAI/MiniMax-M2.1
# NEBIUS_FALLBACK_MODEL=zai-org/GLM-4.7-FP8  # Auto-fallback on any primary model error
# Other options: zai-org/GLM-4.7-FP8, deepseek-ai/DeepSeek-R1-0528,
#   nvidia/Llama-3.1-Nemotron-Ultra-253B-v1, moonshotai/Kimi-K2-Instruct
# NEBIUS_CONTEXT_WINDOW=128000  # Optional: context window in tokens (default: 128000)
# With 128K tokens, CLI can use ~100K char context vs ~8K for SambaNova

# =====================================================
# Embedding Configuration
# =====================================================
# HuggingFace token for downloading models (optional)
HF_TOKEN=your_huggingface_token_here
