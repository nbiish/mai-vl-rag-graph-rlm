# VL-RAG-Graph-RLM LLMs.txt

Complete architecture document showing how all components integrate.

## Overview

This codebase implements a Vision-Language RAG Graph Recursive Language Model (VL-RAG-Graph-RLM) system that combines:
- **VL**: Vision-Language embeddings (Qwen3-VL-Embedding-2B, 2048 dimensions)
- **RAG**: Retrieval-Augmented Generation (Dense + Keyword hybrid search with RRF)
- **Graph**: Knowledge graph construction from unstructured text
- **RLM**: Recursive Language Model (DeepSeek-V3.1 via SambaNova Cloud)

## Architecture Components

### 1. Core RLM System (src/vl_rag_graph_rlm/rlm_core.py)
- **VLRAGGraphRLM**: Main class combining alexzhang13/rlm and ysz/recursive-llm approaches
- **Recursive Processing**: Processes documents by chunking with depth tracking
- **Safe REPL Execution**: RestrictedPython for secure code exploration
- **Multi-Provider Support**: OpenRouter, OpenAI, Anthropic, Gemini, SambaNova, Nebius, LiteLLM
- **Model Selection Priority**: Code-specified > Environment variable > Hardcoded default
  - SambaNova default: DeepSeek-V3.1 (latest, 200+ tok/sec, 128K context)
  - Alternative: DeepSeek-V3-0324 (250+ tok/sec)

### 2. Client System (src/vl_rag_graph_rlm/clients/)
- **SambaNovaClient**: OpenAI-compatible client for SambaNova Cloud
  - Base URL: https://api.sambanova.ai/v1
  - Model: DeepSeek-V3.1 (latest)
  - Alternative: DeepSeek-V3-0324 (fastest inference)
  - Supports: DeepSeek-R1-0528, DeepSeek-R1-Distill-Llama-70B, Llama-4-Maverick
  - Client Factory: get_client(provider, **kwargs) routes provider names to appropriate client

### 3. Qwen3-VL Vision System (src/vl_rag_graph_rlm/rag/qwen3vl.py)
- **Qwen3VLEmbeddingProvider** (2B params, 2048 dim, local):
  - Supports: text, images (path, URL, PIL), videos
  - embed_text(): Generate text embedding
  - embed_image(): Generate image embedding
  - embed_video(): Generate video embedding (frame sampling, max_frames=64)
  - embed_multimodal(): Combined text + image + video
- **Qwen3VLRerankerProvider** (2B params):
  - Cross-attention based reranking
  - rerank(query, documents): Returns list of (idx, score) tuples
  - Binary classification: Yes/No relevance scoring
- **MultimodalDocument**: Dataclass with id, content, metadata, embedding, image_path, video_path

### 4. RAG System (src/vl_rag_graph_rlm/rag/)
- **SearchResult**: Dataclass with id, content, metadata, semantic_score, keyword_score, composite_score
- **ReciprocalRankFusion**: RRF fusion algorithm (dense + keyword)
  - Formula: score = weight * (1 / (k + rank)), k=60
- **MultiFactorReranker**: Multi-signal reranking
  - Fuzzy (25%), Keyword (25%), Semantic (35%), Length (15%)
- Plus: position bonus, proper noun matching
- **CompositeReranker**: Alternative implementation (similar to Paddle-ERNIE-RAG reranker_v2)
- **HybridSearcher**: Combines dense + keyword search with RRF + reranking
- **ERNIEClient**: Baidu AI Studio embeddings (for MilvusVectorStore)

### 5. Unified Pipeline (src/vl_rag_graph_rlm/pipeline.py)
- **MultimodalRAGPipeline**: High-level API combining all components
- Features:
  - Qwen3-VL embeddings (local, free)
  - Hybrid search (dense + keyword + RRF)
  - Qwen3-VL reranking (cross-attention)
  - RLM with recursive reasoning (VLRAGGraphRLM)
  - Multi-provider LLM support (OpenRouter, OpenAI, Anthropic, Gemini)
  - OCR support: Paddle OCR for PDF images, pdf2image fallback
  - Document types: PDF (w/ images), PPTX, TXT, MD
  - Storage: Optional persistence to JSON file

### 6. CLI Tool (src/vrlmrag.py)
- **Entry Point**: vrlmrag (defined in pyproject.toml)
- **Main Function**: run_sambanova_analysis(input_path, query, output)
- Features:
  - Document processing: PPTX via python-pptx (slide text, images)
  - Qwen3-VL integration: Local multimodal embeddings and reranker
  - Knowledge graph: Extracted via DeepSeek-V3.1
  - Reranking: CompositeReranker with multi-signal scoring
  - Context budgeting: 8K chars per call (for free tier 200K TPD)
  - Markdown report generation

## SambaNova Configuration

### Available Models
| Model ID | Context | Speed |
|-----------|---------|-------|
| DeepSeek-V3.1 | 128K tokens | 200+ tok/sec (latest) |
| DeepSeek-V3-0324 | 128K tokens | 250+ tok/sec (fastest) |
| DeepSeek-R1-0528 | 128K tokens | Reasoning |

### Rate Limits (Free Tier)
- **Requests per minute**: 20 RPM
- **Requests per day**: 40 RPD
- **Tokens per day**: 200K TPD

### Environment Variables
```bash
export SAMBANOVA_API_KEY=your_key_here
export SAMBANOVA_MODEL=DeepSeek-V3.1  # Optional: defaults to DeepSeek-V3.1
```

## Qwen3-VL Dependencies

### Required for Vision Support
```toml
[project.dependencies]
# Core ML
torch = ">=2.0.0"
transformers = ">=4.40.0"
qwen-vl-utils = ">=0.0.8"

# Vision
torchvision = ">=0.10.0"

# Optional: PDF processing
[paddleocr]
paddlenlp = ">=2.6.0"
```

### Installed Components Status
| Component | Status |
|-----------|--------|
| torch | ✅ 2.10.0-cp312-arm64 (MPS available) |
| transformers | ✅ 5.1.0 |
| qwen-vl-utils | ✅ 0.0.14 |
| torchvision | ✅ 0.25.0 |
| pillow | ✅ 12.1.0 (pre-installed) |
| python-pptx | ✅ 1.0.2 (pre-installed) |

## End-to-End Test Results

Tested: examples/Writing Tutorial 2022.pptx

| Metric | Result |
|-------|--------|
| Document Intake | 1 PPTX → 20 chunks |
| Qwen3-VL Embeddings | 20 docs embedded (2048 dim each) |
| Hybrid Search | Dense: 20 + Keyword: 20 → RRF: 20 → Qwen3-VL Reranker: Top 15 → Top 5 |
| Knowledge Graph | 20 key concepts extracted from 8K char context |
| Query Responses | 2 queries answered successfully |
| Total Time | 47.73s |
| Model | DeepSeek-V3.1 |
| Embeddings Store | examples/.vrlmrag_store/embeddings.json (735KB persisted) |

## Key Changes Made

1. **Fixed Model Name Bug**: Changed all `DeepSeek-V3.2` references to `DeepSeek-V3.1`
   - Files updated: vrlmrag.py, openai_compatible.py, rlm_core.py, clients/__init__.py, .env.example, all templates, all test files
   - Reason: `DeepSeek-V3.2` doesn't exist on SambaNova

2. **Fixed Token Limit Handling**: Corrected misunderstanding of SambaNova limits
   - Old: Comment "8K context", context truncated to 1200-1500 chars
   - New: Comment "200K TPD", context budgeted to 8K chars per call
   - Changed: Top 1 chunk → Top 3 chunks for richer context

3. **Integrated Qwen3-VL Full Pipeline** into CLI
   - Added: Qwen3-VL embeddings (text + images in unified vector space)
   - Added: Qwen3-VL reranker (cross-attention relevance scoring)
   - Added: MultimodalVectorStore persistence (embeddings.json)
   - Replaced: Simple CompositeReranker with full Qwen3-VL pipeline

4. **Fixed Config Compatibility**: Added check for `config.text_config.hidden_size`
   - Transformers 5.x moved `hidden_size` under text_config subconfig
   - Added fallback handling for both config locations

5. **Updated Document Templates**: Enhanced CLI to show full architecture
   - Report now includes: embedding model, reranker model, embedded count
   - Sources show Qwen3-VL relevance scores with metadata

## Usage Examples

### Basic CLI Usage
```bash
# Run with default queries
vrlmrag --samba-nova examples/Writing Tutorial 2022.pptx

# Run with custom query
vrlmrag --samba-nova examples/Writing Tutorial 2022.pptx -q "What are the key writing techniques?"

# Save to file
vrlmrag --samba-nova examples/Writing Tutorial 2022.pptx -o report.md
```

### Full Pipeline Usage (from Python)
```python
from vl_rag_graph_rlm.pipeline import MultimodalRAGPipeline

# Initialize pipeline with Qwen3-VL
pipeline = MultimodalRAGPipeline(
    llm_provider="sambanova",
    llm_model="DeepSeek-V3.1",
    embedding_model="Qwen/Qwen3-VL-Embedding-2B",
    use_reranker=True,
    storage_path="examples/.vrlmrag_store/embeddings.json"
)

# Add documents
pipeline.add_presentation("my_presentation.pptx", extract_images=True)

# Query
result = pipeline.query("Explain Figure 3")

# Generate report
print(result.answer)
print(f"Cost: ${result.total_cost:.4f}")
```

## Token Budgeting Strategy (Free Tier)

With SambaNova free tier (200K TPD):
- **Knowledge Graph**: 8K chars (~2K tokens) per call
- **Query Context**: 8K chars (~2K tokens) per call (top 3 reranked chunks)
- **Total for 2 queries**: ~4K tokens (~2% of daily budget)
- **Recommendation**: Upgrade to Developer tier (12K RPD, no TPD limit) for production usage.

## Summary

The VL-RAG-Graph-RLM system is now fully integrated with:
✅ Correct SambaNova model (DeepSeek-V3.1)
✅ Proper token limit handling (200K TPD budget, 8K char context per query)
✅ Full Qwen3-VL embeddings and reranker pipeline
✅ Vision-Language support for text + images in unified vector space
✅ Cross-attention reranking for relevance scoring
✅ Persistent embeddings storage (JSON format)
✅ Complete CLI tool with markdown report generation

The system processes documents end-to-end: Document Intake → Qwen3-VL Embeddings → Hybrid Search → Qwen3-VL Reranking → DeepSeek-V3.1 RLM → Knowledge Graph → Markdown Report
